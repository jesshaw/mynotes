# ElasticSearch调优方法

ElasticSearch 调优是为了提升搜索性能和集群的稳定性，通常需要从以下几个方面进行优化：

## 1. **索引设计优化**

- **文档建模**：避免将不相关的数据放在同一个索引中。设计索引时，保证文档结构清晰，字段尽可能简单。
- **分片数量**：分片数量不宜过多或过少。每个分片应承载合理的文档数量，默认的分片数量为 5，但实际中可以根据索引的数据量调整。一般来说，一个分片大小控制在 20GB 以内是较为合理的。
- **副本分片**：根据读请求量调整副本数量。副本不仅提高了读性能，还增加了数据的容错能力，但副本过多会增加存储开销和集群同步负载。
- **合并小文档**：将小文档合并为更大的文档，减少文档数量。ElasticSearch 对每个文档都有元数据和索引数据存储，过多的小文档会消耗大量内存和磁盘空间。

## 2. **查询优化**

- **使用过滤**：尽可能使用 `filter` 代替 `query`，因为 `filter` 不涉及评分计算，性能更高。同时 `filter` 结果可以被缓存以提升后续查询速度。
- **避免深度分页**：深度分页（如 `from` 和 `size` 参数过大）会影响查询性能，因为 ElasticSearch 需要扫描和排序大量数据。可以使用 `search_after` 或 `scroll` 来优化大分页的性能。
- **精简返回字段**：使用 `_source` 参数限制返回的字段，避免返回不必要的字段来减少网络传输负载和解析开销。
- **批量查询**：将多个小的查询合并成批量查询，减少请求次数和响应延迟。

## 3. **集群配置调优**

- **JVM 堆内存设置**：ElasticSearch 使用 JVM，合理的堆内存设置对性能至关重要。一般推荐最大堆内存设置为机器内存的 50% 左右，但不要超过 32GB，因为超过这个值会禁用 JVM 的压缩指针（Compressed Oops），导致内存管理效率降低。
- **缓存配置**：合理设置查询缓存（Query Cache）和字段数据缓存（Fielddata Cache）。可以通过调整 `indices.queries.cache.size` 和 `indices.fielddata.cache.size` 来控制内存中用于缓存查询结果和字段数据的大小。
- **线程池和队列**：调整 ElasticSearch 处理不同任务的线程池（如搜索、索引等）的大小和队列长度，防止任务堆积或队列过长影响性能。可以通过调整 `thread_pool` 相关配置来优化并发性能。

## 4. **硬件和资源优化**

- **磁盘 IO 优化**：ElasticSearch 对磁盘 IO 敏感，推荐使用 SSD 而不是 HDD，SSD 提供更好的随机读取和写入性能。
- **磁盘容量规划**：避免磁盘使用率超过 80%，过度使用磁盘会影响性能，ElasticSearch 在高负载下还可能出现集群节点被标记为只读。
- **内存和 CPU 规划**：确保有足够的 CPU 核心和内存。ElasticSearch 依赖于大量的内存来缓存数据，内存不足会导致频繁的垃圾回收（GC），影响集群性能。

## 5. **垃圾回收（GC）优化**

- **监控 GC**：长时间的垃圾回收会导致节点响应缓慢甚至停止工作，定期监控 JVM 的垃圾回收时间，避免 Full GC 过于频繁。可以通过日志查看 GC 情况，必要时调整堆内存或 GC 策略。
- **启用 G1 GC**：ElasticSearch 推荐使用 G1 GC，它在处理大堆内存时性能更优，可以减少 Full GC 的频率和暂停时间。

## 6. **分布式架构优化**

- **负载均衡**：合理分布节点的角色（如主节点、数据节点、协调节点）。集群中协调节点负责处理查询和请求分发，数据节点负责存储和处理数据，主节点负责维护集群状态。可以将协调节点和主节点分开部署，避免单节点过载。
- **跨集群复制（CCR）**：对跨数据中心的 ElasticSearch 部署，可以使用跨集群复制，减少远程数据同步的延迟和压力。

## 7. **监控与维护**

- **监控集群状态**：使用 ElasticSearch 提供的集群监控工具（如 Kibana 的 Monitoring 插件）来监控集群的健康状态、节点负载、查询延迟、内存使用等关键指标。及时处理磁盘满、节点失效等问题。
- **定期进行索引优化**：对频繁写入和更新的索引，定期执行 `forcemerge` 来减少段（segment）的数量，从而提高查询性能。

## 8. **索引刷新与合并**

- **控制刷新间隔**：ElasticSearch 默认每秒刷新一次索引，可以根据写入速率和应用需求适当延长 `refresh_interval` 时间，减少磁盘 I/O。写入频繁时可以暂时关闭刷新，批量写入完成后再手动刷新。
- **定期合并段（Segment）**：段过多会影响查询性能，通过 `force merge` 或者让 ElasticSearch 自动合并小段，减少段的数量，提升查询速度。

通过结合这些方法，ElasticSearch 的搜索性能、资源利用率和集群稳定性都能得到显著提升。

## 具体做法如下示例

### 1、设计阶段

- 根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引。
- 使用别名进行索引管理。
- 每天凌晨定时对索引做 force_merge 操作，以释放空间。
- 采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink操作，以缩减存储。
- 采取 curator 进行索引的生命周期管理。
- 仅针对需要分词的字段，合理的设置分词器。
- Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。

### 2、写入阶段

- 写入前副本数设置为 0。
- 写入前关闭 refresh_interval 设置为-1，禁用刷新机制。
- 写入过程中：采取 bulk 批量写入。
- 写入后恢复副本数和刷新间隔。
- 尽量使用自动生成的 id。

### 3、查询阶段

- 禁用 wildcard。
- 禁用批量 terms（成百上千的场景）。
- 充分利用倒排索引机制，能 keyword 类型尽量 keyword。
- 数据量大时候，可以先基于时间敲定索引再检索。
- 设置合理的路由机制。

## 4、建立索引阶段的性能提升方法

1. **使用 SSD 存储介质**：提升读写速度。
2. **批量请求并调整大小**：每次批量请求的数据大小为 5-15 MB 是一个不错的起始点。
3. **关闭副本**：在大批量导入数据时，可以通过设置 `index.number_of_replicas: 0` 暂时关闭副本以提升性能。
4. **延长刷新间隔**：如果对实时性要求不高，可以将 `index.refresh_interval` 设置为 30 秒，以减少写入开销。
5. **调整段合并速度**：默认合并速率为 20 MB/s。如果使用 SSD，可提升至 100-200 MB/s。若在进行大量数据导入且无需查询，可暂时关闭合并限流。
6. **增加 translog 阈值**：将 `index.translog.flush_threshold_size` 从默认的 512 MB 增加至 1 GB 或更高，减少磁盘 I/O 次数。

## 5、深度分页与滚动搜索

### 1. 深度分页

- 深度分页是指查询大量页码时的性能问题，例如第 10000 页或第 20000 页，内存和 CPU 消耗会显著增加。而且 Elasticsearch 默认不支持超过一万条记录的分页查询。
- 解决方案：避免深度分页，限制分页的页数展示（例如最多展示 100 页），从而减少对系统性能的影响。

### 2. 滚动搜索（Scroll）

- 当需要查询超过 1 万条数据时，可以使用滚动搜索（Scroll）。该方法可以分批次查询数据，避免一次性加载过多数据导致性能问题。
- 滚动搜索的过程：
    1. 第一次查询时生成一个滚动 ID，用作锚点。
    2. 后续查询通过该滚动 ID 继续从上次查询的位置开始，直到获取所有数据。
    3. 滚动搜索基于数据的快照，因此查询过程中数据的变动不会影响查询结果。
